---
title: 'Assignment 1: Individual Project | EC349 Data Science For Economists (2023/24)'
author: "u2129775"
date: "2023-12-05"
output:
  html_document: default
  pdf_document: default
---

> Tabula statement

We're part of an academic community at Warwick.

Whether studying, teaching, or researching, we’re all taking part in an expert conversation which must meet standards of academic integrity. When we all meet these standards, we can take pride in our own academic achievements, as individuals and as an academic community.

Academic integrity means committing to honesty in academic work, giving credit where we've used others' ideas and being proud of our own achievements.

In submitting my work I confirm that:

1. I have read the guidance on academic integrity provided in the Student Handbook and understand the University regulations in relation to Academic Integrity. I am aware of the potential consequences of Academic Misconduct.

2. I declare that the work is all my own, except where I have stated otherwise.

3. No substantial part(s) of the work submitted here has also been submitted by me in other credit bearing assessments courses of study (other than in certain cases of a resubmission of a piece of work), and I acknowledge that if this has been done this may lead to an appropriate sanction.

4. Where a generative Artificial Intelligence such as ChatGPT has been used I confirm I have abided by both the University guidance and specific requirements as set out in the Student Handbook and the Assessment brief. I have clearly acknowledged the use of any generative Artificial Intelligence in my submission, my reasoning for using it and which generative AI (or AIs) I have used. Except where indicated the work is otherwise entirely my own.

5. I understand that should this piece of work raise concerns requiring investigation in relation to any of points above, it is possible that other work I have submitted for assessment will be checked, even if marks (provisional or confirmed) have been published.

6. Where a proof-reader, paid or unpaid was used, I confirm that the proofreader was made aware of and has complied with the University’s proofreading policy.

7. I consent that my work may be submitted to Turnitin or other analytical technology. I understand the use of this service (or similar), along with other methods of maintaining the integrity of the academic process, will help the University uphold academic standards and assessment fairness.

Privacy statement

The data on this form relates to your submission of coursework. The date and time of your submission, your identity, and the work you have submitted will be stored. We will only use this data to administer and record your coursework submission.

Related articles

Reg. 11 Academic Integrity (from 4 Oct 2021)

Guidance on Regulation 11

Proofreading Policy  

Education Policy and Quality Team

Academic Integrity (warwick.ac.uk)

\newpage


> Introduction

Social networks and websites such as Yelp are becoming increasingly popular as we advance into a digitalised era. Nowadays, the growing reliance of individuals on these sites to make decisions indicates an upward trend to confront the relevance of user reviews and ratings. When navigating on Yelp, a business’ star rating is indisputably a decisive factor that users consider when making consumption decisions. Therefore, this paper aims to leverage the data features extracted from Yelp datasets to train, test the model, and predict the star ratings that individuals assign to businesses. This is achieved via the employment of Random Forest and consecutive assessment of the model’s performance.

The data science methodology adopted in this paper is the widely implemented Cross-Industry Standard Process for Data Mining (CRISP-DM) methodology, a framework introduced in 1999, for its flexibility and adaptability to data science projects. It also allows beginner data scientists undertake research via a standardised structure. Additionally, from the comparative study of data mining process models, Shafique and Qaiser (2014) implicitly suggest how CRISP-DM outstands similar data science methodologies such as KDD and SEMMA by including all steps with fewer phases, streamlining the process. However, as previously outlined in the lectures, there is a potential inefficiency in teamwork or when working with stakeholders. Nonetheless, since this report consists of an individual task, these challenges can be neglected.

Overall, this report is structured as follows: starting with this Introduction or Business Understanding, Dataset Characteristics (with Data Understanding and Data Preparation), Modeling, and Evaluation. Finally, we conclude by summarising the limitations and challenges found during the research. Like many other papers, due to unclear instructions for future use, this paper will exclude the deployment phase.

```{r,echo=FALSE, results='hide', message=FALSE, warning=FALSE}
options(repos = c(CRAN = "https://cran.rstudio.com/"))

library(ggplot2)
library(lattice)
library(caret)
library(glmnet)
library(tm)
library(dplyr)
library(tidyverse)
library(lubridate)
library(stringr)
library(DT)      
library(leaflet) 
library(SnowballC) 
library(textcat)
library(corrplot)
library(knitr)
library(scales)
library(randomForest)
```

> Dataset Characteristics

```{r, echo=FALSE, results='hide'}
#Set working directory
setwd("/Users/shirleyzhu/Desktop/EC349 Assignment 1/Datasets")
load("yelp_review_small.rda") 
load("yelp_user_small.rda") 
```

In this research, we employed two Yelp datasets with 1398056 reviews given by 397579 users, accessible from the Yelp website. Both the user data and the review data are in rda format.

For an overview of the features, we merged the user data and the review data by matching the ‘user_id’ data frames to obtain the ‘merged_data’, encompassing review ratings, user characteristics, and engagement variables. Additionally, the columns that might be most influential to user ratings were selected and a correlation matrix for all these features in the merged_data was imputed accordingly (Figure 3), resulting in a dataset comprised of 279878 observations and 19 variables.

```{r merged_data, echo=TRUE}
merged_data <- review_data_small %>%
  inner_join(user_data_small, by = "user_id")
```

```{r, echo=FALSE}
str(merged_data)
```


```{r, echo=FALSE, results='hide'}
#Select columns that might be relevant
merged_data <- merged_data %>%
  select(stars, review_count, useful.x, funny.x, cool.x, 
         elite, fans, average_stars, compliment_hot, compliment_more, 
         compliment_profile, compliment_cute, compliment_list, compliment_note, 
         compliment_plain, compliment_cool, compliment_funny, compliment_writer, 
         compliment_photos)
```

```{r obs merged_data, echo=FALSE, results='hide'}
dim(merged_data) #279878 observations and 19 variables
```

Figure 1 illustrates the distribution of user review counts on a logarithmic scale to deal with the disparate orders of magnitude, extending from users with very few reviews to those with thousands and more:

```{r , echo=FALSE, results='hide'}
#Count the number who gave reviews and the number of users who didn't
users_without_reviews <- merged_data %>%
  filter(review_count == 0)
users_with_reviews <- merged_data %>%
  filter(review_count > 0)
```

```{r summary_table, echo=FALSE}
#Create a summary table for review_count
summary_table <- data.frame(
  Category = c("Users with Reviews", "Users without Reviews"),
  Count = c(nrow(users_with_reviews), nrow(users_without_reviews))  )
```

```{r kable_summary, echo=FALSE}
kable(summary_table, format = "markdown")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Distribution of review count and plot a pie chart to illustrate
ggplot(merged_data, aes(x = review_count)) +
  geom_histogram(binwidth = 1, fill = "tomato", color = "black") +
  labs(title = "Figure 1. Distribution Review Count (in logs)",
       x = "Number of Reviews (in log)",
       y = "Number of Users")  +
  scale_x_log10() +
  theme_minimal() +
  scale_y_continuous(labels = comma_format())
```

The histogram shows that most users contribute a limited number of reviews, with around 40,000 users in the peak frequency category (log 1). Around 116,000 individuals engage actively in the long tail effect (log (10)). Logically, the higher the logarithmic ranges (log (100) and (log (1000)), the lower the review counts. Four users without reviews were excluded from the plot since it is not possible to take logs of values of 0. 

Key data preprocessing steps consists of the following: First, no missing data was found. 

```{r, echo=TRUE}
sum(is.na(merged_data))
```

Second, for the variable ‘elite’, missing values were transformed into a binary numeric variable equal to 1 if a non-empty string, 0 otherwise, followed by the conversion from categorical to a numerical feature.

```{r, echo=FALSE, results='hide'}
#'elite'
merged_data$elite <- ifelse(merged_data$elite != "", 1, 0)
merged_data$elite <- as.numeric(merged_data$elite)
```

```{r, echo=TRUE}
str(merged_data$elite)
```

To our interest, even though the ‘compliment’ variables have many values equal to 0, it is sensible to keep these as they may contain relevant information. Consequently, we want to build a model that both ensures robustness and detects key characteristics of the data in times when users received no compliments.

We then look at the distribution of review ratings. Figure 2 reveals a moderate skewness of the review distributions to the 4- and 5-star categories. 

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
##Distribution of stars in merged_data
table(merged_data$stars)
ggplot(merged_data, aes(x = stars, fill = stars)) +
  geom_bar(color = "black") +
  geom_text(
    stat = "count",
    aes(label = after_stat(count)),
    position = position_stack(vjust = 0.5),
    color = "black",
    size = 3 ) +
  labs(title = "Figure 2. Distribution of Stars Rating", x = "Stars", y = "Number of Reviews") +
  theme_minimal() +
  scale_y_continuous(labels = comma_format())

summary(merged_data$stars)
```

Woolf (2014) corroborated to this result by elucidating that Yelp reviews tend to exhibit optimistic bias in the long run. According to Cunningham (2021), this may lead to a “positivity problem” for consumers who rely on the business’ star ratings to make consumption decisions, which requires careful attention.

Then, a correlation matrix has been plotted to interpret different patterns in merged_data. 

```{r, echo=FALSE, results='hide'}
#Calculate the correlation matrix
cor_matrix <- cor(merged_data[, sapply(merged_data, is.numeric)])
print(cor_matrix)
```

```{r, echo=FALSE}
#Visual representation of the correlation matrix
corrplot(cor_matrix, method = "color", tl.col = "black", tl.cex = 0.7)
title("Figure 3. Correlation Matrix")
```

The interpretation of the heatmap begins by interpreting the large number of coefficients with values close to zero that potentially evidence non-linear patterns in the data. We can also infer a positive correlation of 0.58 between ‘stars’ (review ratings), and ‘average_stars’ (average stars rating of a user), such that higher star ratings are linked to better user interactions. Furthermore, a positive relationship between popularity (‘fans’) with the number of reviews (‘review_count’) for a coefficient of 0.66, and engagement (‘compliment_note’, ‘compliment_plain’, ‘compliment_funny’ for coefficients of 0.53, 0.67, 0.60 respectively, just to name a few) can also be deduced from the heatmap.

To select the features to fit into the predictive model, we utilised the Recursive Feature Elimination or RFE with cross-validation, since this predictor selection method decreases overfitting, thereby increasing accuracy (Gomede, 2023). RFE fits the model across all columns of the data, then prunes features with less ‘weight’ at each iteration, leaving us with the optimal subset of predictors. This method’s efficiency has been further corroborated by Guyon et al. (2002) in their study of gene selection for cancer classification. 

```{r, echo=TRUE, results='hide'}
#First convert the dependent variable (stars) to factor type
merged_data$stars <- as.factor(merged_data$stars)
```

```{r, echo=FALSE, results='hide'}
set.seed(123)  
sampled_data <- merged_data[sample(nrow(merged_data), 2000), ] #create sample to increase speed

#Employment of the recursive feature elimination to select the relevant features
control <- rfeControl(functions=rfFuncs, method="cv", number=5)
results <- rfe(sampled_data[, -which(names(sampled_data) == "stars")], sampled_data$stars, sizes=c(1:5), rfeControl=control)
```

> Modelling

To predict the number of stars a user has given to another business, we trained the random forest model after a thorough consideration of the efficiency of the model in preventing overfitting and performing well when coping with large datasets. The resulting optimal five features are ‘average_stars’, ‘fans’, ‘review_count’, ‘useful.x’, and ‘compliment_note’ (specific meanings can be found in the R script)

```{r, echo=FALSE}
print(results)

#Select optimal features
opt_features <- results$optVariables
opt_features
```

According to the requirements of this assignment, we split merged user-reviews data into training and testing data, both with 19 variables and 160003 and 39997 observations, respectively, following the 80-20 rule, which is commonly accredited as the split that yields the best results (Gholamy et al., 2018).

```{r, echo=FALSE, results='hide'}
#Split into testing and training data
set.seed(1)
data <- merged_data[sample(nrow(merged_data), 200000), ] #sample a subset of 200,000 observations increase the speed in training the model.
splitIndex <- createDataPartition(data$stars, p = 0.8, list = FALSE) #since the merged data has 279878 obs, we can increase the number of observations while generating the testing data to enhance performance.
training_data <- data[splitIndex, ]
testing_data <- data[-splitIndex, ]
```

```{r, echo=TRUE}
dim(testing_data)
dim(training_data) 
```

> Evaluation

Consequently, a random forest model has been trained using the ‘best’ predictors derived from RFE. This approach was supplemented by cross-validation to ensure robustness throughout.

```{r, echo=FALSE}
# Train the Random Forest model
rf_model <- randomForest(stars ~ ., data = training_data[, c(opt_features, "stars")], ntree = 100, mtry = 3) #100 trees, three features at each split. 
```

```{r, echo=FALSE}
print(rf_model)
```

```{r, echo=FALSE}
# Predict the number of stars
predictions <- predict(rf_model, testing_data[, opt_features])

# Evaluate the predictions derived from model
confusionMatrix(predictions, testing_data$stars)

```

From the table we can infer moderate accuracy in the model performance with an estimated error rate of 45.2%. Besides, the statistical outcome from the confusion matrix evidence that the applied machine learning model yielded correct predictions for around 54.93% of the overall data. Another  key statistic is the Kappa values of 0.2929, suggesting a fair reliability, according to Landis and Koch’s (1977) interpretation. Therefore, the model demonstrated fluctuating sensitivity and specificity across different star classes or ratings, which define the true positive and true negative rate respectively, excelling in the prediction of higher star ratings, e.g., 5-star ratings, with 84.16% sensitivity. Thus, we conclude that the model predicts stars reasonably, but could possibly be enhanced for 2 and 3-star ratings.


> Conclusion

As a result of the increasing popularity of social networks like Yelp, researchers have widely applied datasets to establish nexus between attributes that are influential in the number of stars users give to businesses. Nevertheless, the large size of the provided files has substantially increased the time needed to preprocess the data, select the optimal variables, and train the random forest model. Results point to a moderate prediction accuracy of 54.93%, which calls for further optimisation of the predicting model. Some further steps could be either including sentiment analysis to extrapolate the meaning behind text reviews, investigating the relevance of features from the other three Yelp datasets, experimenting with other models, or a combination of all three.

> References

Cunningham, K. (2021) The positivity problem: Why online star ratings are too good to be true, The Guardian. Available at: https://www.theguardian.com/lifeandstyle/2021/apr/21/the-positivity-problem-why-online-star-ratings-are-too-good-to-be-true (Accessed: 04 December 2023). 

Gholamy, A., Kreinovich, V. and Kosheleva, O. (2018) Why 70/30 or 80/20 relation between training and testing sets: A pedagogical explanation, ScholarWorks@UTEP. Available at: https://scholarworks.utep.edu/cs_techrep/1209/#:~:text=We%20first%20train%20our%20model,of%20the%20data%20for%20training. (Accessed: 04 December 2023). 

Gomede, E. (2023) Recursive feature elimination: A powerful technique for feature selection in machine learning, Medium. Available at: https://medium.com/@evertongomede/recursive-feature-elimination-a-powerful-technique-for-feature-selection-in-machine-learning-89b3c2f3c26a#:~:text=We%20create%20a%20Random%20Forest,it%20to%20the%20training%20data. (Accessed: 04 December 2023). 

Guyon, I. et al. (2002) Gene selection for cancer classification using support Vector Machines - machine learning, SpringerLink. Available at: https://link.springer.com/article/10.1023/A:1012487302797 (Accessed: 04 December 2023). 

Landis, J.R. and Koch, G.G. (1977) The measurement of observer agreement for categorical data - JSTOR. Available at: https://www.jstor.org/stable/2529310 (Accessed: 04 December 2023). 
Shafique, U. and Qaiser, H. (2014) A comparative study of data mining process models (KDD, CRISP-DM and Semma). Available at: https://www.researchgate.net/publication/268770881_A_Comparative_Study_of_Data_Mining_Process_Models_KDD_CRISP-DM_and_SEMMA (Accessed: 04 December 2023). 

Woolf, M. (2014) The statistical difference between 1-Star and 5-star reviews on yelp, Max Woolf’s Blog (Alt + H). Available at: https://minimaxir.com/2014/09/one-star-five-stars/ (Accessed: 04 December 2023). 
